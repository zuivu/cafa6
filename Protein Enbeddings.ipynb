{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd92ea61-1798-40a2-bb7b-8eb4aefaca3a",
     "showTitle": true,
     "title": "Installation"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install biopython\n",
    "!pip install h5py\n",
    "!pip install torch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c8f3e4d-2345-40a2-bb7b-8eb4aefaca3b",
     "showTitle": true,
     "title": "Import"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from Bio import SeqIO\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e6f7d8-3456-40a2-bb7b-8eb4aefaca3c",
     "showTitle": true,
     "title": "Load ProtTrans Model"
    }
   },
   "outputs": [],
   "source": [
    "# Specify your model name or path\n",
    "# For Hugging Face models, use the model ID (e.g., \"Rostlab/prot_t5_xl_uniref50\")\n",
    "# For local models, use the local path\n",
    "model_name = \"Rostlab/prot_t5_xl_uniref50\"  # or use local path like \"./embedding_model/Rostlab_prot_t5_xl_uniref50\"\n",
    "\n",
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "\n",
    "print(f\"Loading model {model_name}...\")\n",
    "model = T5EncoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded successfully on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a174448-1c2f-4593-98e5-3c9fe3c89039",
     "showTitle": true,
     "title": "Read Fasta"
    }
   },
   "outputs": [],
   "source": [
    "# Path to your FASTA file\n",
    "fasta_path = \"train_sequences.fasta\"  # Update this path\n",
    "\n",
    "# Read all sequences\n",
    "sequences = []\n",
    "for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "    sequences.append((record.id, str(record.seq)))\n",
    "\n",
    "print(f\"Total sequences loaded: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5295ba5-edcd-4170-840c-1e16f4fc133e",
     "showTitle": true,
     "title": "Function to Extract Embeddings in Batches"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    \"\"\"\n",
    "    Prepare protein sequence for ProtTrans model.\n",
    "    ProtT5 expects sequences with spaces between amino acids.\n",
    "    \"\"\"\n",
    "    # Add space between each amino acid\n",
    "    return \" \".join(list(seq))\n",
    "\n",
    "\n",
    "def extract_embeddings_batch_with_progress(sequence_list, model, tokenizer, device, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extract embeddings for a batch of sequences with progress bar\n",
    "    \n",
    "    Args:\n",
    "        sequence_list: List of tuples (id, sequence)\n",
    "        model: ProtTrans T5 model\n",
    "        tokenizer: T5 tokenizer\n",
    "        device: torch device\n",
    "        batch_size: Number of sequences to process at once\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with sequence IDs as keys and embeddings as values\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    total_batches = (len(sequence_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=len(sequence_list), desc=\"Processing sequences\", \n",
    "                unit=\"seq\", ncols=100)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(sequence_list), batch_size):\n",
    "        batch = sequence_list[i:i+batch_size]\n",
    "        \n",
    "        # Prepare sequences (add spaces between amino acids)\n",
    "        batch_ids = [seq_id for seq_id, _ in batch]\n",
    "        batch_seqs = [prepare_sequence(seq) for _, seq in batch]\n",
    "        \n",
    "        # Tokenize sequences\n",
    "        ids = tokenizer(batch_seqs, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "        input_ids = ids['input_ids'].to(device)\n",
    "        attention_mask = ids['attention_mask'].to(device)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        # Per-sequence mean pooling (average over sequence length)\n",
    "        for j, (seq_id, seq) in enumerate(batch):\n",
    "            # Get sequence length (excluding padding)\n",
    "            seq_len = attention_mask[j].sum().item()\n",
    "            \n",
    "            # Mean pooling over sequence length (excluding padding and special tokens)\n",
    "            # Note: ProtT5 adds special tokens, so we take mean over valid positions\n",
    "            seq_embedding = embeddings[j, :seq_len].mean(dim=0)\n",
    "            \n",
    "            embeddings_dict[seq_id] = seq_embedding.cpu().numpy()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(len(batch))\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    pbar.close()\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e8f9d0-4567-40a2-bb7b-8eb4aefaca3d",
     "showTitle": true,
     "title": "Handle Different Sequence Lengths"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def group_sequences_by_length(sequences, length_bins=[100, 500, 1000, 5000, 10000, 20000]):\n",
    "    \"\"\"Group sequences into bins based on length\"\"\"\n",
    "    grouped = defaultdict(list)\n",
    "    \n",
    "    for seq_id, seq in sequences:\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        # Find appropriate bin\n",
    "        bin_idx = 0\n",
    "        for i, bin_size in enumerate(length_bins):\n",
    "            if seq_len <= bin_size:\n",
    "                bin_idx = i\n",
    "                break\n",
    "        else:\n",
    "            bin_idx = len(length_bins)  # Sequences longer than max bin\n",
    "        \n",
    "        grouped[bin_idx].append((seq_id, seq))\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "# Group sequences\n",
    "grouped_sequences = group_sequences_by_length(sequences)\n",
    "\n",
    "# Print distribution\n",
    "print(\"Sequence length distribution:\")\n",
    "bins = [100, 500, 1000, 5000, 10000, 20000]\n",
    "for idx in sorted(grouped_sequences.keys()):\n",
    "    if idx < len(bins):\n",
    "        print(f\"  Bin {idx} (\u2264{bins[idx]} aa): {len(grouped_sequences[idx])} sequences\")\n",
    "    else:\n",
    "        print(f\"  Bin {idx} (>{bins[-1]} aa): {len(grouped_sequences[idx])} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d8e9f0-5678-40a2-bb7b-8eb4aefaca3e",
     "showTitle": true,
     "title": "Save Embeddings Function"
    }
   },
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings_dict, output_dir, bin_idx=None, is_cumulative=False, is_final=False):\n",
    "    \"\"\"\n",
    "    Save embeddings with proper naming\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if is_final:\n",
    "        filename = \"embeddings_final\"\n",
    "    elif is_cumulative:\n",
    "        filename = \"embeddings_cumulative\"\n",
    "    elif bin_idx is not None:\n",
    "        filename = f\"embeddings_bin_{bin_idx}\"\n",
    "    else:\n",
    "        filename = \"embeddings\"\n",
    "    \n",
    "    # Save as pickle\n",
    "    pkl_path = os.path.join(output_dir, f\"{filename}.pkl\")\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(embeddings_dict, f, protocol=4)\n",
    "    \n",
    "    # Save as npz for easier loading in other tools\n",
    "    npz_path = os.path.join(output_dir, f\"{filename}.npz\")\n",
    "    np.savez_compressed(npz_path, **embeddings_dict)\n",
    "    \n",
    "    return pkl_path, npz_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e9f0a1-6789-40a2-bb7b-8eb4aefaca3f",
     "showTitle": true,
     "title": "Load Progress Function"
    }
   },
   "outputs": [],
   "source": [
    "def load_progress(output_dir):\n",
    "    \"\"\"\n",
    "    Load previous progress if exists\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(output_dir, \"progress_metadata.pkl\")\n",
    "    cumulative_path = os.path.join(output_dir, \"embeddings_cumulative.pkl\")\n",
    "    \n",
    "    if os.path.exists(metadata_path) and os.path.exists(cumulative_path):\n",
    "        try:\n",
    "            with open(metadata_path, 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "            \n",
    "            with open(cumulative_path, 'rb') as f:\n",
    "                embeddings = pickle.load(f)\n",
    "            \n",
    "            return metadata, embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load progress: {e}\")\n",
    "            return None, {}\n",
    "    \n",
    "    return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9f0a1b2-789a-40a2-bb7b-8eb4aefaca3g",
     "showTitle": true,
     "title": "Set Output Directory and Check for Previous Progress"
    }
   },
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "output_dir = \"./extracted_embeddings/ProtTrans-embeddings/\"\n",
    "\n",
    "# Try to load previous progress\n",
    "print(\"Checking for previous progress...\")\n",
    "metadata, all_embeddings = load_progress(output_dir)\n",
    "\n",
    "if metadata:\n",
    "    print(f\"\u2713 Found previous progress!\")\n",
    "    print(f\"  Last completed bin: {metadata['last_bin']}\")\n",
    "    print(f\"  Sequences processed: {metadata['processed_sequences']}/{metadata['total_sequences']}\")\n",
    "    print(f\"  Timestamp: {metadata['timestamp']}\")\n",
    "    start_bin = metadata['last_bin'] + 1\n",
    "else:\n",
    "    print(\"No previous progress found. Starting from scratch.\")\n",
    "    all_embeddings = {}\n",
    "    start_bin = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a1b2c3-89ab-40a2-bb7b-8eb4aefaca3h",
     "showTitle": true,
     "title": "Define Batch Sizes"
    }
   },
   "outputs": [],
   "source": [
    "# Process each group with appropriate batch size\n",
    "# ProtTrans models typically need smaller batch sizes than ESM due to larger model size\n",
    "batch_sizes = {\n",
    "    0: 32,   # Short sequences (\u2264100 aa)\n",
    "    1: 8,    # Medium sequences (\u2264500 aa)\n",
    "    2: 4,    # Long sequences (\u22641000 aa)\n",
    "    3: 2,    # Very long sequences (\u22645000 aa)\n",
    "    4: 1,    # Extremely long sequences (\u226410000 aa)\n",
    "    5: 1,    # Ultra long sequences (\u226420000 aa)\n",
    "    6: 1     # Longest sequences (>20000 aa)\n",
    "}\n",
    "\n",
    "total_sequences = sum(len(seqs) for seqs in grouped_sequences.values())\n",
    "\n",
    "print(f\"Total sequences to process: {total_sequences}\")\n",
    "print(f\"Starting from bin: {start_bin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b2c3d4-9abc-40a2-bb7b-8eb4aefaca3i",
     "showTitle": true,
     "title": "Main Processing Loop - Save After Each Bin"
    }
   },
   "outputs": [],
   "source": [
    "for bin_idx, seqs in sorted(grouped_sequences.items()):\n",
    "    # Skip already processed bins\n",
    "    if bin_idx < start_bin:\n",
    "        print(f\"Skipping Bin {bin_idx} (already processed)\")\n",
    "        continue\n",
    "    \n",
    "    batch_size = batch_sizes.get(bin_idx, 1)\n",
    "    total_batches = (len(seqs) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BIN {bin_idx}: Processing {len(seqs)} sequences\")\n",
    "    print(f\"Batch size: {batch_size} | Total batches: {total_batches}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Extract embeddings for this bin\n",
    "    bin_embeddings = extract_embeddings_batch_with_progress(\n",
    "        seqs, model, tokenizer, device, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Merge with cumulative embeddings\n",
    "    all_embeddings.update(bin_embeddings)\n",
    "    \n",
    "    # Save bin-specific results\n",
    "    print(f\"\\n\ud83d\udcbe Saving bin {bin_idx} results...\")\n",
    "    pkl_path, npz_path = save_embeddings(bin_embeddings, output_dir, bin_idx=bin_idx)\n",
    "    print(f\"\u2713 Bin {bin_idx} saved: {pkl_path}\")\n",
    "    \n",
    "    # Save cumulative results\n",
    "    print(f\"\ud83d\udcbe Saving cumulative results...\")\n",
    "    pkl_path, npz_path = save_embeddings(all_embeddings, output_dir, is_cumulative=True)\n",
    "    print(f\"\u2713 Cumulative embeddings saved: {pkl_path}\")\n",
    "    \n",
    "    # Save metadata for progress tracking\n",
    "    metadata = {\n",
    "        'last_bin': bin_idx,\n",
    "        'processed_sequences': len(all_embeddings),\n",
    "        'total_sequences': total_sequences,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    metadata_path = os.path.join(output_dir, \"progress_metadata.pkl\")\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(f\"\u2713 Progress: {len(all_embeddings)}/{total_sequences} sequences processed\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del bin_embeddings\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\u2705 All bins processed successfully!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2c3d4e5-abcd-40a2-bb7b-8eb4aefaca3j",
     "showTitle": true,
     "title": "Final Save"
    }
   },
   "outputs": [],
   "source": [
    "# Final save\n",
    "print(\"\ud83d\udcbe Creating final output files...\")\n",
    "pkl_path, npz_path = save_embeddings(all_embeddings, output_dir, is_final=True)\n",
    "print(f\"\u2713 Final embeddings saved:\")\n",
    "print(f\"  - {pkl_path}\")\n",
    "print(f\"  - {npz_path}\")\n",
    "print(f\"\\n\u2705 Processing complete! Total embeddings: {len(all_embeddings)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Protein Enbeddings",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}